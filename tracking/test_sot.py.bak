import _init_paths
import os
import cv2
import torch
import random
import argparse
import numpy as np
from tqdm import tqdm
from easydict import EasyDict as edict
from os.path import exists, join, dirname, realpath

import tracker.sot_tracker as tracker_builder
import utils.model_helper as loader
import utils.box_helper as boxhelper
import utils.log_helper as recorder
import utils.sot_builder as builder
import utils.read_file as reader
import search.model_derived as model_derived
from dataset.benchmark_loader import load_sot_benchmark as datafactory
from models.densesiaminference import DenseSiamInference
import pickle


def parse_args():
    parser = argparse.ArgumentParser(description='Test SOT trackers')
    parser.add_argument('--cfg', type=str, default='../experiments/DenseSiam.yaml', help='yaml configure file name')
    parser.add_argument('--resume',  default=None, help='resume checkpoin, if None, use resume in config or epoch testing')
    parser.add_argument('--dataset',  default=None, help='evaluated benchmark, if None, use that in config')
    parser.add_argument('--arch_type', type=str, default='t', help='retrain densesiam from which type')
    parser.add_argument('--vis', default=False, type=bool, help='visualization')
    parser.add_argument('--video_path', default=None, help='whether run on a single video (.mp4 or others)')
    parser.add_argument('--video', default=None, type=str, help='eval one special video')

    args = parser.parse_args()

    return args


def track(inputs):
    siam_tracker = inputs['tracker']
    siam_net = inputs['network']
    video_info = inputs['video_info']
    args = inputs['args']
    config = inputs['config']

    start_frame, lost, toc, boxes, times = 0, 0, 0, [], []

    # save result to evaluate
    result_path, time_path = recorder.sot_benchmark_save_path(config, args, video_info)
    if os.path.exists(result_path):
        return  # for mult-gputesting

    image_files, gt = video_info['image_files'], video_info['gt']

    for f, image_file in enumerate(image_files):
        im = cv2.imread(image_file)
        if len(im.shape) == 2: im = cv2.cvtColor(im, cv2.COLOR_GRAY2BGR)

        tic = cv2.getTickCount()
        if f == start_frame:  # init
            if 'UAVTRACK112' in config.TEST.DATA or config.TEST.DATA == 'ANTIUAV' or config.TEST.DATA == 'UAVDARK135':
                init_rect = np.array(gt[f])
            else:
                init_rect = gt[f]
            cx, cy, w, h = boxhelper.get_axis_aligned_bbox(init_rect)  # center_x, center_y
            target_pos, target_sz = np.array([cx, cy]), np.array([w, h])

            init_inputs = {'image': im, 'pos': target_pos, 'sz': target_sz, 'model': siam_net}
            siam_tracker.init(init_inputs)  # init tracker

            # location = cxy_wh_2_rect(state['target_pos'], state['target_sz'])
            boxes.append(1 if 'VOT' in config.TEST.DATA else init_rect)

            times.append(0.02)  # just used for testing on online saver which requires time recording, e.g. got10k

        elif f > start_frame:  # tracking
            state = siam_tracker.track(im)

            location = boxhelper.cxy_wh_2_rect(state['pos'], state['sz'])
            b_overlap = boxhelper.poly_iou(gt[f], location) if 'VOT' in config.TEST.DATA else 1
            times.append(0.02)
            if b_overlap > 0:
                boxes.append(location)
            else:
                boxes.append(2)
                start_frame = f + 5
                lost += 1
        else:
            boxes.append(0)

        toc += cv2.getTickCount() - tic

    save_inputs = {'boxes': boxes,
                   'times': times,
                   'result_path': result_path,
                   'time_path': time_path,
                   'args': args,
                   'config': config
                   }
    recorder.sot_benchmark_save(save_inputs)

    toc /= cv2.getTickFrequency()
    # print('Video: {:12s} Time: {:2.1f}s Speed: {:3.1f}fps  Lost {}'.format(video_info['name'], toc, f / toc, lost))
    return f / toc


def main():
    print('===> load config <====')
    args = parse_args()
    if args.cfg is not None:
        config = edict(reader.load_yaml(args.cfg))
    else:
        raise Exception('Please set the config file for tracking test!')

    # create derived model of super model
    with open('./arch/arch_{}.pkl'.format(str(args.arch_type)), 'rb') as f:
        arch_dict = pickle.load(f)
    arch_config = arch_dict['arch']
    derivedNetwork = getattr(model_derived, '%s_Net' % config.MODEL.NAME.upper())
    der_Net = lambda net_config: derivedNetwork(net_config, config=config)
    derived_model = der_Net(arch_config)

    # create model
    print('====> build model <====')
    siam_net = DenseSiamInference(derived_model, config)

    # load checkpoint
    print('===> init Siamese <====')
    if args.resume is None or args.resume == 'None':
        resume = config.TEST.RESUME
    else:
        resume = args.resume
    siam_net = loader.load_pretrain(siam_net, resume, print_unuse=False)
    siam_net.eval()
    siam_net = siam_net.cuda()

    # create tracker
    siam_tracker = tracker_builder.SiamTracker(config)

    # prepare video
    if args.dataset is None:
        dataset_loader = datafactory(config.TEST.DATA)
    else:
        config.TEST.DATA = args.dataset
        dataset_loader = datafactory(args.dataset)
    dataset = dataset_loader.load()
    video_keys = list(dataset.keys()).copy()

    fps_recorder = []
    for video in video_keys:
        if args.video is None or (args.video is not None and args.video == video):
            inputs = {'tracker': siam_tracker,
                      'network': siam_net,
                      'video_info': dataset[video],
                      'args': args,
                      'config': config
                      }
            fps = track(inputs)
            fps_recorder.append(fps)
    print('Dataset: {:12s} Speed: {:3.1f}fps'.format(config.TEST.DATA, np.average(fps_recorder)))


if __name__ == '__main__':
    main()

